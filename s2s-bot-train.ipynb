{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oELknLtqLpM",
        "outputId": "465db4f7-6130-4a85-f247-021ead3a3cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get -y install mecab libmecab-dev mecab-ipadic-utf8\n",
        "\n",
        "!pip install pysrt\n",
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install mecab-python3\n",
        "\n",
        "# Set the MECABRC environment variable\n",
        "import os\n",
        "os.environ['MECABRC'] = \"/etc/mecabrc\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P2xNF14MvBNV",
        "outputId": "048d8fe6-720f-4dd4-f665-3ff55459819c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,031 kB]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,396 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,278 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,648 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,161 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,451 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,598 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,370 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,194 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:22 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:23 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.9 kB]\n",
            "Fetched 26.6 MB in 4s (7,227 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-utils\n",
            "0 upgraded, 6 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 7,367 kB of archives.\n",
            "After this operation, 59.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmecab2 amd64 0.996-14build9 [199 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmecab-dev amd64 0.996-14build9 [306 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-utils amd64 0.996-14build9 [4,850 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-ipadic all 2.7.0-20070801+main-3 [6,718 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mecab amd64 0.996-14build9 [136 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-3 [4,384 B]\n",
            "Fetched 7,367 kB in 1s (5,080 kB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-14build9_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-14build9) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-14build9_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-14build9_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../3-mecab-ipadic_2.7.0-20070801+main-3_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-3) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../4-mecab_0.996-14build9_amd64.deb ...\n",
            "Unpacking mecab (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../5-mecab-ipadic-utf8_2.7.0-20070801+main-3_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-3) ...\n",
            "Setting up libmecab2:amd64 (0.996-14build9) ...\n",
            "Setting up libmecab-dev (0.996-14build9) ...\n",
            "Setting up mecab-utils (0.996-14build9) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-3) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-14build9) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-3) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import des bibliothèques nécessaires\n",
        "import os\n",
        "import pysrt\n",
        "import re\n",
        "\n",
        "# Chemin vers le fichier .srt\n",
        "DATA_FOLDER = \"/content/data_srt\"\n",
        "DATA_SRT = [os.path.join(DATA_FOLDER, f) for f in os.listdir(DATA_FOLDER) if f.endswith('.srt')]\n",
        "output_file_path = \"/content/data_qa.txt\"  # Chemin du fichier de sortie\n",
        "output_data = 0\n",
        "\n",
        "# Fonction pour nettoyer le texte : supprimer les balises, symboles et parenthèses\n",
        "def clean_text(text):\n",
        "    # Supprime les balises comme {\\\\an8}, les notes musicales ♪ et les parenthèses ()\n",
        "    cleaned = re.sub(r'\\{.*?\\}|♪|[\\（\\(].*?[\\）\\)]|～|⸺', '', text)\n",
        "    cleaned = re.sub(r'<.*?>', '', cleaned)\n",
        "    cleaned = cleaned.replace('\\n', ' ')  # Remplace les retours à la ligne par un espace\n",
        "    return cleaned.strip()  # Supprime les espaces en trop\n",
        "\n",
        "# Fonction pour ignorer les interjections ou phrases très courtes\n",
        "def is_valid_text(text):\n",
        "    # On ignore les interjections ou les phrases très courtes qui n'ont pas de sens complet\n",
        "    if len(text) < 3:  # Ignore les chaînes de moins de 3 caractères\n",
        "        return False\n",
        "    # Ignore certaines interjections communes ou symboles\n",
        "    if re.match(r\"^[あっ！ふっ！たあっ！-]+$\", text):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    for srt_file in DATA_SRT:\n",
        "        # Ouverture du fichier .srt\n",
        "        subs = pysrt.open(srt_file)\n",
        "\n",
        "        # Boucle pour extraire et nettoyer les sous-titres\n",
        "        for idT in range(len(subs) - 1):  # On itère sur tous les sous-titres, sauf le dernier\n",
        "            # Extraction et nettoyage du texte du sous-titre\n",
        "            question = clean_text(subs[idT].text)\n",
        "            response = clean_text(subs[idT + 1].text)\n",
        "\n",
        "            # Vérification si les textes sont valides\n",
        "            if is_valid_text(question) and is_valid_text(response):\n",
        "                # Formater la ligne à écrire\n",
        "                line = f\"{question}\\t{response}\\n\"\n",
        "\n",
        "                # Vérifier que la ligne a le bon format avant de l'écrire\n",
        "                if line.strip() and \"\\t\" in line:  # Vérifie qu'il y a quelque chose après le tab\n",
        "                    output_file.write(line)\n",
        "                    output_data += 1\n",
        "\n",
        "# Afficher le résultat nettoyé\n",
        "print(f\"Résultat nettoyé : {output_data} messages.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBRLYE9br0Pa",
        "outputId": "376e7be4-b5fa-4d76-b5e7-50d32442c4d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultat nettoyé : 20605 messages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import onnx\n",
        "import onnxruntime as rt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import MeCab\n",
        "import re\n",
        "\n",
        "# Check GPU availability and set device accordingly\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    device = '/GPU:0'  # Use GPU if available\n",
        "else:\n",
        "    device = '/CPU:0'  # Fallback to CPU\n",
        "\n",
        "print(\"Device :\",device)\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "\n",
        "  def tokenize_japanese(text):\n",
        "      \"\"\"\n",
        "      Tokenizes Japanese text using MeCab.\n",
        "\n",
        "      Args:\n",
        "          text: The input Japanese text.\n",
        "\n",
        "      Returns:\n",
        "          A list of tokens.\n",
        "      \"\"\"\n",
        "      tagger = MeCab.Tagger(\"-Owakati\")\n",
        "      wakati_text = tagger.parse(text)\n",
        "      tokens = wakati_text.strip().split()\n",
        "      return tokens\n",
        "\n",
        "\n",
        "\n",
        "  # 1. Préparation des données\n",
        "  data_path = '/content/data_qa.txt'\n",
        "\n",
        "  with open(data_path, 'r', encoding='utf-8') as f:\n",
        "      lines = f.read().split('\\n')\n",
        "\n",
        "  # Tokenisation et création du vocabulaire\n",
        "  vocab = set()\n",
        "  encoder_input_data = []\n",
        "  decoder_input_data = []\n",
        "  decoder_target_data = []\n",
        "\n",
        "  for line in lines:\n",
        "      if line:\n",
        "          input_text, target_text = line.split('\\t')\n",
        "\n",
        "          input_tokens = tokenize_japanese(input_text)\n",
        "          target_tokens = tokenize_japanese(target_text)\n",
        "\n",
        "          encoder_input_data.append(input_tokens)\n",
        "          decoder_input_data.append(['<start>'] + target_tokens)\n",
        "          decoder_target_data.append(target_tokens + ['<end>'])\n",
        "\n",
        "          vocab.update(input_tokens + target_tokens)\n",
        "\n",
        "  # Add <start> and <end> tokens to vocab before creating word_to_index\n",
        "  vocab.add('<start>')\n",
        "  vocab.add('<end>')\n",
        "\n",
        "  # Création d'index pour le vocabulaire\n",
        "  word_to_index = {token: index for index, token in enumerate(vocab)}\n",
        "  index_to_word = {index: token for token, index in word_to_index.items()}\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  # Padding des séquences\n",
        "  max_encoder_seq_length = max(len(seq) for seq in encoder_input_data)\n",
        "  max_decoder_seq_length = max(len(seq) for seq in decoder_input_data)\n",
        "\n",
        "  encoder_input_data = [[word_to_index[token] for token in seq] + [0] * (max_encoder_seq_length - len(seq)) for seq in encoder_input_data]\n",
        "  decoder_input_data = [[word_to_index[token] for token in seq] + [0] * (max_decoder_seq_length - len(seq)) for seq in decoder_input_data]\n",
        "  decoder_target_data = [[word_to_index[token] for token in seq] + [0] * (max_decoder_seq_length - len(seq)) for seq in decoder_target_data]\n",
        "\n",
        "  encoder_input_data = np.array(encoder_input_data)\n",
        "  decoder_input_data = np.array(decoder_input_data)\n",
        "  decoder_target_data = np.array(decoder_target_data)\n",
        "\n",
        "  # 2. Création du modèle Seq2Seq avec LSTM et régularisation\n",
        "  embedding_dim = 256\n",
        "  units = 512\n",
        "\n",
        "  encoder_inputs = keras.Input(shape=(None,))\n",
        "  encoder_embedding = keras.layers.Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "  encoder_lstm = keras.layers.LSTM(units, return_state=True, dropout=0.2, recurrent_dropout=0.2)  # Dropout ajouté\n",
        "  encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "  encoder_states = [state_h, state_c]\n",
        "\n",
        "  decoder_inputs = keras.Input(shape=(None,))\n",
        "  decoder_embedding = keras.layers.Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "  decoder_lstm = keras.layers.LSTM(units, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)  # Dropout ajouté\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "  decoder_dense = keras.layers.Dense(vocab_size, activation='softmax', kernel_regularizer=keras.regularizers.l2(0.01))  # Régularisation L2 ajoutée\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  # Compilation et entraînement du modèle avec ajustement des hyperparamètres\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Taux d'apprentissage ajusté\n",
        "  model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  model.fit([encoder_input_data, decoder_input_data], decoder_target_data, epochs=20, batch_size=32)  # Époques et taille du lot ajustées\n",
        "\n",
        "# Enregistrez le modèle Keras\n",
        "model.save(\"s2s_bot_mobile_ja.h5\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IWX2OEurJN_",
        "outputId": "966eecea-22c0-4b36-9269-6f415872e278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device : /GPU:0\n",
            "Epoch 1/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 118ms/step - accuracy: 0.7659 - loss: 2.7343\n",
            "Epoch 2/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 119ms/step - accuracy: 0.7921 - loss: 1.6088\n",
            "Epoch 3/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 120ms/step - accuracy: 0.7953 - loss: 1.5896\n",
            "Epoch 4/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 121ms/step - accuracy: 0.7966 - loss: 1.5680\n",
            "Epoch 5/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 122ms/step - accuracy: 0.7982 - loss: 1.5554\n",
            "Epoch 6/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 120ms/step - accuracy: 0.8031 - loss: 1.5144\n",
            "Epoch 7/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 122ms/step - accuracy: 0.8030 - loss: 1.5189\n",
            "Epoch 8/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 122ms/step - accuracy: 0.8044 - loss: 1.4966\n",
            "Epoch 9/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 120ms/step - accuracy: 0.8053 - loss: 1.4940\n",
            "Epoch 10/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 119ms/step - accuracy: 0.8074 - loss: 1.4727\n",
            "Epoch 11/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 120ms/step - accuracy: 0.8082 - loss: 1.4700\n",
            "Epoch 12/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 119ms/step - accuracy: 0.8091 - loss: 1.4649\n",
            "Epoch 13/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 119ms/step - accuracy: 0.8102 - loss: 1.4552\n",
            "Epoch 14/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 121ms/step - accuracy: 0.8128 - loss: 1.4323\n",
            "Epoch 15/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 119ms/step - accuracy: 0.8142 - loss: 1.4272\n",
            "Epoch 16/20\n",
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 120ms/step - accuracy: 0.8142 - loss: 1.4233\n",
            "Epoch 17/20\n",
            "\u001b[1m466/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 122ms/step - accuracy: 0.8154 - loss: 1.4145"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "jzi9_JYSxjof",
        "outputId": "9a49e3b8-28c1-4e04-fa6a-523ba53aaa04"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m2,706,176\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m2,706,176\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m1,574,912\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),    │      \u001b[38;5;34m1,574,912\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10571\u001b[0m)    │      \u001b[38;5;34m5,422,923\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,706,176</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,706,176</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10571</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,422,923</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m41,955,299\u001b[0m (160.05 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,955,299</span> (160.05 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,985,099\u001b[0m (53.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,985,099</span> (53.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m27,970,200\u001b[0m (106.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,970,200</span> (106.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input_text, temperature=2.0):\n",
        "    \"\"\"Generates a response for a given input text.\n",
        "\n",
        "    Args:\n",
        "        input_text: The input text as a string.\n",
        "\n",
        "    Returns:\n",
        "        The generated response as a string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    input_tokens = tokenize_japanese(input_text)\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    input_seq = [word_to_index[token] for token in input_tokens if token in word_to_index]\n",
        "\n",
        "    # Pad the input sequence\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_encoder_seq_length, padding='post')\n",
        "\n",
        "    # Get the initial decoder state\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Initialize the target sequence with the start token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word_to_index['<start>']\n",
        "\n",
        "    # Generate tokens until the end token is reached\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        # Apply temperature to softmax\n",
        "        output_tokens = output_tokens[0, -1, :] / temperature\n",
        "        sampled_token_index = np.argmax(output_tokens)\n",
        "\n",
        "        # Convert the token index to word\n",
        "        sampled_token = index_to_word[sampled_token_index]\n",
        "\n",
        "        # If the end token is reached, stop generating\n",
        "        if sampled_token == '<end>':\n",
        "            break\n",
        "\n",
        "        # Append the predicted token to the decoded sentence\n",
        "        decoded_sentence += sampled_token + ' '\n",
        "\n",
        "        # Update the target sequence and decoder state\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# Create inference models\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = keras.Input(shape=(units,))\n",
        "decoder_state_input_c = keras.Input(shape=(units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Example usage\n",
        "input_text = \"番ステキな季節が来た\"\n",
        "response = generate_response(input_text)\n",
        "print(\"Question :\",input_text)\n",
        "print(\"Response :\",response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjBdLMK8zXXt",
        "outputId": "a446b53a-3dcd-4c5e-deb9-5115146ea195"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Question : 番ステキな季節が来た\n",
            "Response : 私 は 何 を し てる の ？\n"
          ]
        }
      ]
    }
  ]
}